## Goal: Understand LLama2 everything

### Starting points
1. about the [LLaMa2](https://arxiv.org/pdf/2307.09288.pdf)
2. about the [Chinese LLaMa2 Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)
3. about the [LLaMa2](https://github.com/FlagAlpha/Llama2-Chinese)

### Steps
- read the paper
  - Not well understanded technical terms:
    - gradient clipping
    - weight decay
    - zero out 
    - Rjection Sampling
  -  Iterative Fine-Tuning (newly proposed RL process in LLaMa2)
    - 


### Questions:

1. why LLama2 choose Alibi?
2. how to do the pre-training?
3. what's the LLama2 Chinese Alpaca



### Reference
- [TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION](https://arxiv.org/pdf/2108.12409.pdf)
- []()

