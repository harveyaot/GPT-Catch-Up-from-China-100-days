## Goal : Efficient transformers
- Context Length
- Attention Merchanism


## Studies
- Linear Attension
- Weight quantization

## Papers

- [REFORMER: THE EFFICIENT TRANSFORMER](https://arxiv.org/pdf/2001.04451.pdf)
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf)
- [Transformer Dissection: A Unified Understanding of Transformerâ€™s Attention via the Lens of Kernel](https://arxiv.org/pdf/1908.11775.pdf)
