# GPT-Catch-Up-from-China-100-days
An individual developer located in China, what' the roadmap looks like to catch up the milestones of GPT evolution in future 100 days
  - [Day 1-4](./Day1-4.md): Target Ran LLAMA on Chinese Datest with prompt and fully open sourcing on it.   Because of Qingming (Pure Lightness) vaction, got 4 days working on this. 
  - [Day 5](./Day5.md) Targeting learning the the key technologies behind the ChatGPT majorly from the the recent talks from Andrej Karpthy [state of GPT](https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg) and makin plan of the mimic pretrain experiment.
  - [Day 6-9](./Day6-9.md) Targeting rampup with huggingface, gpt and run locally 
  - [Day 10-11](./Day10-11.md) Targeting Lora, and 6B model training with V100/A100 remotely
  - [Day 12](./Day12.md) Targeting inference and demo solutions.
  - [Day 13](./Day13.md) Targeting multi-targets and multi-nodes solution.
  - [Day 14](To Add) Target to research on the RHLF process.
  - [Day 15](To Add) Target to research on the LLama-2
  - [Day 16](To Add) Target to research on the streaming inference
  
