# GPT-Catch-Up-from-China-100-days
An individual developer located in China, what' the roadmap looks like to catch up the milestones of GPT evolution in future 100 days
  - [Day 1-4](./Day1-4.md): Target Ran LLAMA on Chinese Datest with prompt and fully open sourcing on it.   Because of Qingming (Pure Lightness) vaction, got 4 days working on this. 
  - [Day 5](./Day5.md) Targeting learning the the key technologies behind the ChatGPT majorly from the the recent talks from Andrej Karpthy [state of GPT](https://mp.weixin.qq.com/s/zmEGzm1cdXupNoqZ65h7yg) and makin plan of the mimic pretrain experiment.
  - [Day 6-9](./Day6-9.md) Targeting rampup with huggingface, gpt and run locally 
  - [Day 10-11](./) Targeting Lora, and 6B model training with V100/A100 remotely

