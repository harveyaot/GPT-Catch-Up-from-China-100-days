### Goal
1. Build an live e2e LLM App, and make it publicly
2. Verify the Tecnical problems
    - pretrain
    - SFT
    - Reinforcement Learning (P1)
    - Inference
    - Multi-Modality (P2)
    

3. Resolve a realistic issue and problem. 

### Goal Breakdown

1. overall the ?
    - (NLP/Image Tasks)Tooling. translation, summarization, writing assitant, image recognition  
    - (Search engine) Knowledge search, summarizate the knowledge related the question
    - (Agent) Bot: chat bot, guidence bot, gaming agent. AutoGPT (P2)
    - To Add More

2. what model would like to use?
    1. Of course a smaller one, 1B-7B. GPT2, or smaller  LLama2, [TinyLlama](https://github.com/jzhang38/TinyLlama)
    2. Endpoint using hugging face's solution.
    3. serverlize need to output a solution.



### Pretraining

1. Training Data: SlimPajama
    -  From [Celebras](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)
    - :cherries: how they performed the data cleaning and deduplication? 
        - After removing punctuation, space symbols, newlines and tabs, we filtered out documents with less than 200 characters
        - To perform deduplication we used MinHashLSH Leskovec et al. (2014) with a Jaccard similarity threshold of 0.8
        -  Construct document signatures on top of pre-processed lower-cased 
        13-grams
    - :cherries: how the processing pipeline looks like?
        - [github](https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama)
        - 2.5 days, 64 core CPU, largest memory 1.4TB

    - :cherries: SFT data: 
        - 
2. Efficency Part
    - FSDF, Xformers, FlashAttenstions. 
    
