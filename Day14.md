## Goal
To understand below point:
1. PPO and TRPO, the fundermentals
2. How it trained on LLM
3. research on the RLHF benifits and how to use it


## Details

### About the PPO and TRPO:

1. The `Actor-Critic` is a combination of value-based, and policy-based methods where the Actor controls how our agent behaves using the Policy gradient, and the Critic evaluates how good the action taken by the Agent based on value-function.
2. Understatnd how ChatGPT trained a Rewards model and how the the RL learning objective and it support contious training for rewards and RL, but what's the connect with PPO?
3. 





# Reference

- [ChatGPT RL Process](https://dida.do/blog/chatgpt-reinforcement-learning)
- [Training language models to follow instructions
 with human feedback](https://arxiv.org/pdf/2203.02155.pdf)

